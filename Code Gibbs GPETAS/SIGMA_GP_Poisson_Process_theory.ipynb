{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90f33ece",
   "metadata": {},
   "source": [
    "# Bayesian Calibration of a SIGMA-GP Poisson process using Gibbs Sampling\n",
    "\n",
    "M. Keller, I. Seydi, 2025\n",
    "\n",
    "## SIGMA-GP generative model\n",
    "\n",
    "Suppose we observe a dataset $D :$\n",
    "$$\n",
    "D := \\{(x_i, y_i)\\}_{1\\leq i\\leq N}\n",
    "$$\n",
    "which follows a time-homogeneous Poisson process over a certain period of time $T$ and a certain 2D domain $\\Omega \\subset \\mathbb R^2,$ with a non-homogeneous spatial intensity given by: $\\lambda(x,y)$ such that:\n",
    "$$\n",
    "N \\vert \\lambda \\sim \\mathcal P(\\Lambda\\times T) \n",
    "$$\n",
    "$$\n",
    "\\left[D \\vert N, \\lambda\\right] = \\Lambda^{-N}\\prod_{i=1}^N  \\lambda\\left(x_i, y_i \\right)\n",
    "$$\n",
    "where: $\\Lambda = \\int_{x,y \\in \\Omega} \\lambda(x, y)dxdy.$ Remember the likelihood is then given by:\n",
    "$$\n",
    "\\left[D , N \\vert \\lambda\\right] = \\frac{\\exp\\left( - \\Lambda \\times T\\right) \\times T^N}{N!} \\prod_{i=1}^N  \\lambda\\left(x_i, y_i \\right)\n",
    "$$\n",
    "\n",
    "We model $\\lambda$ as:\n",
    "$$\n",
    "\\lambda(x, y) = \\bar\\lambda \\times \\sigma\\circ f(x, y),\n",
    "$$\n",
    "where $\\bar\\lambda$ is known,\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + \\exp(-x)}\n",
    "$$\n",
    "and $f:\\Omega \\mapsto\\mathbb R$ is an unknwon function we wish to estimate. \n",
    "Adopting a Bayesian perspective, we define a Gaussian process prior on $f$ with known mean function $m(x,y)$ and Gaussian covariance function $k((x, y) ; (x',y')):$\n",
    "$$\n",
    "f \\sim GP\\left(m(x,y)\\ ;\\ k((x, y) ; (x',y')) \\right)\n",
    "$$\n",
    "\n",
    "Our goal is to sample from the posterior density of $f$ or, more precisely, of $f_T := \\{f(x,y)\\}_{(x,y) \\in T}$ with $T$ an arbitrary finite set of test points : \n",
    "$$\n",
    "\\left[ f_T \\vert D \\right] \\propto \\left[ D \\vert f_T \\right] \\times \\left[ f_T\\right] \n",
    "$$\n",
    "$$\n",
    " \\propto \\mathcal P\\left( N \\vert \\bar\\lambda\\int_\\Omega \\sigma \\circ f_T\\right) \\times \\left[D \\vert N, \\bar\\lambda \\times \\sigma\\circ f(x, y)\\right] \\times GP\\left( f_T \\vert m(\\cdot ), k(\\cdot, \\cdot)\\right).\n",
    "$$\n",
    "\n",
    "Except in special cases, $\\int_\\Omega \\sigma \\circ f_T$ is untractable and hence so is the target posterior. This is why, following Donner, Opper, Molkenthin etc. we resort to data augmentation as described hereafter.\n",
    "\n",
    "###  Informative prior based on seismotectonical zonings\n",
    "\n",
    "Furthermore, we wish to introduce a prior on the mean function $m(x, y):\\mathbb R^2 \\to \\mathbb R^{+\\star}$ based on a partition \n",
    "$$\n",
    "\\bigsqcup_{j=1}^J S_j = \\Omega \n",
    "$$\n",
    "of the search domain, which identifies zones $S_j$ of similar intensities. We use these as factors in the following linear decomposition of the GP mean function :\n",
    "$$\n",
    "m(x, y) := \\sum_{j=1}^J \\epsilon_j {\\bf 1}_{\\{(x, y) \\in S_j\\}} := U(x, y)  \\epsilon,\n",
    "$$\n",
    "with $U(x, y) = ({\\bf 1}_{\\{(x, y) \\in S_j\\}})_{j=1:J} \\in \\{0, 1\\}^J $\n",
    "We then define a prior covariance matrix on the regressor vector ${\\bf\\epsilon} = (\\epsilon_1, \\ldots, \\epsilon_J)$:\n",
    "$$\n",
    "{\\bf\\epsilon} \\sim \\mathcal N_J\\left( 0, \\Sigma_\\epsilon  \\right).\n",
    "$$\n",
    "Here we choose to model the covariance between a pair of zones $(S_j, S_{j'})$ as a function of the difference between the corresponding centers of mass $(c_j - c_{j'}),$ with \n",
    "$$\n",
    "c_j = \\frac{\\int_{(x,y)\\in S_j}(x,y)dxdy}{\\int_{(x,y)\\in S_j}dxdy}\n",
    "$$ \n",
    "so that:\n",
    "$$\n",
    "\\Sigma_\\epsilon = \\left(  k_\\epsilon(c_j, c_{j'}) \\right)_{1\\leq j,j'\\leq J}.\n",
    "$$\n",
    "\n",
    "#### Integrating $\\epsilon$ out\n",
    "\n",
    "Note that $\\bf\\epsilon$ can be integrated out completely from the joint density of the couple $(\\bf\\epsilon, f),$ by remarking that the above-defined process $m(x,y)$ is Gaussian, as a pointwise linear transform of a Gaussian vector, with mean and covariance functions given by:\n",
    "$$\n",
    "m(x, y) \\sim GP \\left( 0\\ ;\\ U(x, y)^\\top \\Sigma_\\epsilon U(x', y') \\right),\n",
    "$$\n",
    "Hence, this amounts to re-defining the GP covariance kernel by adding a non-stationary term:\n",
    "$$\n",
    "f \\sim GP\\left(0\\ ;\\ k((x, y) ; (x',y')) + U(x, y)^\\top \\Sigma_\\epsilon U(x', y') \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2883e84-c650-46ff-9fe6-df6752c16a5c",
   "metadata": {},
   "source": [
    "## Latent Poisson Process\n",
    "\n",
    "First, we introduce $\\Pi=\\{(X_j, Y_j)\\}_{1\\leq j\\leq N_\\Pi}$ a latent random variable, defined as the realization of a second Poisson process which is time-homogeneous over the same period of time $T,$ and spatially non homogeneous over the same 2D domain $\\Omega \\subset \\mathbb R^2,$ with intensity $\\check\\lambda(x,y)$ such that:\n",
    "$$\n",
    "\\check\\lambda(x,y) := \\bar\\lambda - \\lambda(x,y) = \\bar\\lambda\\left(1- \\sigma\\left\\{ f(x, y)\\right\\}\\right) = \\bar\\lambda\\sigma\\left\\{ -f(x, y)\\right\\}.\n",
    "$$\n",
    "Notice that the superposition $D \\cup \\Pi,$ with size $N_{tot} := N+N_\\Pi,$ follows a spatially homogeneous Poisson process since:\n",
    "$$\n",
    "\\lambda(x,y) + \\check\\lambda(x,y) = \\lambda(x,y) + \\bar\\lambda - \\lambda(x,y) = \\bar\\lambda,\n",
    "$$\n",
    "Hence \n",
    "$$\n",
    "N_{tot}:=N+N_\\Pi \\sim \\mathcal P\\left( \\bar\\lambda \\times T \\times \\vert\\Omega\\vert \\right).\n",
    "$$\n",
    "Hence $N_{tot}$ is easy to simulate, following the above Poisson law. $\\Pi$ is also easy to simulate given $f,$ by thinning: simply draw $N_{tot}$ uniform 2D points $(X_j, Y_j)$ within $\\Omega,$ compute $f_j := f(X_j, Y_j)$ then allocate each point $j$ to $\\Pi$ with probability:\n",
    "$$\n",
    "\\mathbb P\\left[  (X_j, Y_j) \\in \\Pi \\vert f_j \\right] = \\frac{\\sigma\\left\\{ -f_j\\right\\}}{\\sigma\\left\\{ -f_j\\right\\} + \\sigma\\left\\{ f_j\\right\\}} = \\sigma\\left\\{ -f_j\\right\\}.\n",
    "$$\n",
    "Once this is done, re-define and re-order the $(X_j, Y_j)$ such that $\\{(X_j, Y_j)\\}_{1\\leq j\\leq N} = D$ and $\\{(X_j, Y_j)\\}_{N\\leq j+1\\leq N_{tot}} = \\Pi$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09b5411-8eec-4360-8f19-03211bab7c16",
   "metadata": {},
   "source": [
    "## Latent Polya-Gamma Process (PGP)\n",
    "\n",
    "We now introduce a second set of latent variables ${\\boldsymbol\\omega} = (\\omega_j)_{1\\leq j\\leq N_{tot}},$ such that, *a priori*:\n",
    "$$\n",
    "\\omega_j \\vert N_{tot} \\stackrel{iid}{\\sim} PG(1, 0)\n",
    "$$\n",
    "and re-define arbitrarily the augmented conditional density of $D \\cup \\Pi $ given $ f, \\boldsymbol\\omega$:\n",
    "$$\n",
    "\\left[ D \\cup \\Pi \\vert (f_j)_{j=1, \\ldots, N_{tot}}, {\\bf\\omega} \\right] = \n",
    "\\frac{\\exp\\left(-\\bar\\lambda\\times T\\times \\vert\\Omega\\vert\\right)}{N_{tot}!}\n",
    "%\\left(\\bar\\lambda\\times T\\times \\vert\\Omega\\vert\\right)^{N_{tot}}\n",
    "\\times\\prod_{j=1}^N \\frac{\\bar\\lambda}{2}\\exp\\left(\\frac{f_j}{2}-\\frac{f_j^2}{2}\\omega_j\\right) \n",
    "\\times \\prod_{j=N+1}^{N_{tot}}\\frac{\\bar\\lambda}{2} \\exp\\left(-\\frac{f_j}{2}-\\frac{f_j^2}{2}\\omega_j\\right),\n",
    "$$\n",
    "then (thanks to elaborate Laplace-transformation wizardry) we can show that, by integrating $\\boldsymbol\\omega$ out, we indeed recover:\n",
    "$$\n",
    "\\left[ D \\cup \\Pi \\vert (f_j)_{j=1, \\ldots, N_{tot}}, N \\right] = \n",
    "\\frac{\\exp\\left(-\\bar\\lambda\\times T\\times \\vert\\Omega\\vert\\right)}{N! (N_{tot}-N)!}\n",
    "\\prod_{j=1}^N \\bar\\lambda \\sigma(f_j) \\times \\prod_{j=N+1}^{N_{tot}}\\bar\\lambda \\sigma(-f_j).\n",
    "$$\n",
    "$$\n",
    "= [D,N \\vert \\lambda] \\times [\\Pi,N_{tot}-N \\vert \\lambda].\n",
    "$$\n",
    "\n",
    "### Conditional conjugacy\n",
    "\n",
    "Plus, the PGP model is conjugate to the SIGMA-GP model, in that the $w_j$'s are *a posteriori* independent conditional on $D\\cup \\Pi$ and the $f_j$, for $j = 1,\\ldots, N_{tot} :$\n",
    "$$\n",
    "w_j \\vert D \\cup \\Pi, f_j \\stackrel{ind}{\\sim} PG(1,\\vert f_j\\vert).\n",
    "$$\n",
    "\n",
    "This property is key to including PGP variables as a block in a Gibbs sampling scheme.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdfb4d5-e510-4606-8a25-f315b4d717d3",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The full data-augmented model contains the following variables, oserved or latent:\n",
    "\n",
    "- $D$: the observed points\n",
    "- $\\Pi$ the latent points\n",
    "- $\\boldsymbol \\omega = \\boldsymbol \\omega_D \\cup \\boldsymbol \\omega_\\Pi$ the latent PGP at observed and latent points\n",
    "- $\\boldsymbol f = \\boldsymbol f_D \\cup \\boldsymbol f_\\Pi$ the latent GP at observed and latent points\n",
    "\n",
    "Then the complete augmented likelihood is given by: \n",
    "$$\n",
    "\\left[ D, \\boldsymbol f, \\boldsymbol \\omega_D, \\Pi,, \\boldsymbol \\omega_\\Pi, N_{tot} \\vert \\bar\\lambda \\right]\n",
    "=\n",
    "\\left[ D \\vert \\boldsymbol f_D, {\\boldsymbol\\omega_D} \\right] \\left[ \\Pi \\vert \\boldsymbol f_\\Pi, {\\boldsymbol\\omega_D} \\right] \\left[ \\boldsymbol f \\right] \\left[{\\boldsymbol\\omega} \\right] \n",
    "$$\n",
    "$$= \n",
    "\\frac{\\exp\\left(-\\bar\\lambda\\times T\\times \\vert\\Omega\\vert\\right)}{N_{tot}!}\n",
    "%\\left(\\bar\\lambda\\times T\\times \\vert\\Omega\\vert\\right)^{N_{tot}}\n",
    "\\times\\prod_{j=1}^N \\frac{\\bar\\lambda}{2}\\exp\\left(\\frac{f_j}{2}-\\frac{f_j^2}{2}\\omega_j\\right) \n",
    "\\times \\prod_{j=N+1}^{N_{tot}}\\frac{\\bar\\lambda}{2} \\exp\\left(-\\frac{f_j}{2}-\\frac{f_j^2}{2}\\omega_j\\right),\n",
    "$$\n",
    "$$\n",
    "\\times \\boldsymbol N\\left( \\boldsymbol f \\vert \\boldsymbol m, \\boldsymbol K \\right) \\times \\prod_{j=1}^{N} PG(\\omega_j \\vert 1, 0) \\times \\prod_{j=N+1}^{N_{tot}} PG(\\omega_j \\vert 1, 0)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd70a348-3e44-487d-96b1-efaef42b9d16",
   "metadata": {},
   "source": [
    "## Blocked Gibbs Algorithm\n",
    "\n",
    "The idea is to divide the above variables into a certain number of blocks, such that the complete conditional density of each block can be simulated exactly. Here are the blocks that we suggest: \n",
    "\n",
    "- $\\bf f := \\boldsymbol f_D \\cup \\boldsymbol f_\\Pi $ the latent GP evaluated at observed and latent points;\n",
    "- $\\Pi$ : the latent Poisson process and the associated latent Polya-Gamma process;\n",
    "- $\\boldsymbol\\omega = \\boldsymbol \\omega_D \\cup \\boldsymbol \\omega_\\Pi $ the latent PGP at the observed and latent points\n",
    "\n",
    "The Gibbs algorithm generates a Markov chain $(\\boldsymbol\\theta^{(t)})_{t=1,\\ldots,T}$ with $\\boldsymbol\\theta^{(t)}:=\\left(\\boldsymbol f^{(t)}, \\Pi^{(t)},\\boldsymbol \\omega^{(t)} \\right),$ given a starting point $\\boldsymbol\\theta^{(1)},$ ideally simulated from the prior law on $\\theta,$ and iterating the following conditional updating steps.\n",
    "\n",
    "### Remark\n",
    "\n",
    "By analyzing the details of the updating steps below, it becomes clear that only the initial values $\\Pi^{(1)}, \\boldsymbol\\omega^{(1)}$ really need to be generated, the remaining components, corresponding to $\\boldsymbol f^{(1)},$ can be set arbitrarily. We suggest to simulate $N_{tot}^{(1)} \\sim \\mathcal P(\\bar\\lambda\\ T\\ \\vert\\Omega\\vert)\\times \\boldsymbol 1_{\\left\\{N_{tot}^{(1)}\\geq N\\right\\}},$ simulate $(X_i^{(1)}, Y_i^{(1)})\\stackrel{iid}{\\sim}\\mathcal U(\\Omega)$ for $i=N+1, \\ldots, N_{tot}$ then set $\\Pi^{(1)}:=\\{(X_i^{(1)}, Y_i^{(1)})\\}_{i=N+1,\\ldots, N_{tot}^{(1)}}$ and finally $\\omega_i^{(1)} \\stackrel{iid}{\\sim} PG(1,0)$ for $i=1,\\ldots, N_{tot}^{(1)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bd0441-9481-443e-8352-0d8df5c292b1",
   "metadata": {},
   "source": [
    "We now assume we have generated $\\boldsymbol \\theta^{(t)}$ for some $t\\geq 1,$ and detail step by step how to obtain the updated vector $\\boldsymbol \\theta^{(t+1)}.$\n",
    "\n",
    "### Updating the latent GP evaluated at all points\n",
    "\n",
    "The latent GP prior is conjugate to the PGP model, so that the conditional posterior used to simulate the updated GP $\\boldsymbol f^{(t+1)}$ is the multivariate normal:\n",
    "$$\n",
    "\\boldsymbol f^{(t+1)} \\vert D, \\Pi^{(t)}, \\boldsymbol \\omega^{(t)}, \\boldsymbol \\epsilon^{(t)} \\sim \\mathcal N\\left(\\left(\\boldsymbol \\Omega^{(t)} + {\\boldsymbol K_{\\boldsymbol f, \\boldsymbol f}^{(t)}}^{-1}\\right)^{-1}\\left( {\\boldsymbol K_{\\boldsymbol f, \\boldsymbol f}^{(t)}}^{-1}\\boldsymbol m^{(t)} + \\boldsymbol u\\ \\right);\\ \\left(\\boldsymbol \\Omega^{(t)} + {\\boldsymbol K_{\\boldsymbol f, \\boldsymbol f}^{(t)}}^{-1}\\right)^{-1}\\right),\n",
    "$$\n",
    "with \n",
    "$$\n",
    "\\boldsymbol \\Omega^{(t)} = diag\\left(\\boldsymbol\\omega^{(t)}\\right),\n",
    "$$ \n",
    "$$\n",
    "\\boldsymbol K_{\\boldsymbol f, \\boldsymbol f}^{(t)}= \\left( k((X_i^{(t)}, Y_i^{(t)}), (X_j^{(t)}, Y_j^{(t)})^{(t)}) \\right)_{\\substack{i=1:N_{tot}^{(t)}\\\\ j=1:N_{tot}^{(t)}}}\n",
    "$$\n",
    "$$\n",
    "\\boldsymbol m^{(t)} = \\boldsymbol U^{(t)} \\boldsymbol \\epsilon^{(t)}\n",
    "$$\n",
    "$$\n",
    "\\boldsymbol U^{(t)} = \\left( \\boldsymbol 1_{S_j}\\left(X_i^{(t)}, Y_i^{(t)}\\right) \\right)_{\\substack{ i=1:N_{tot}^{(t)} \\\\ j=1:J }}\n",
    "$$\n",
    "$$\n",
    "\\boldsymbol X^{(t)}, \\boldsymbol Y^{(t)} = D \\cup \\Pi^{(t)}\n",
    "$$\n",
    "and $\\boldsymbol u^{(t)} := (u_1^{(t)}, \\ldots, u^{(t)}_{N_{tot}^{(t)}}),$ $u_i^{(t)} = \\frac{1}{2}$ if $i \\leq N$ and $u_i^{(t)} = -\\frac{1}{2}$ if $i > N.$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff03042",
   "metadata": {},
   "source": [
    "<!-- #### Adding zoning prior information\n",
    "\n",
    "As explained earlier, taking into account the zoning infromation into the latent GP prior can be done simply by modifying the prior mean and variance function, leading to the following modified conditional posterior:\n",
    "$$\n",
    "\\boldsymbol f^{(t+1)} \\vert D, \\Pi^{(t)}, \\boldsymbol \\omega^{(t)} \\sim \\mathcal N\\left(\\left(\\boldsymbol \\Omega^{(t)} + {\\boldsymbol K_{\\boldsymbol f, \\boldsymbol f}^{(t)}}^{-1}\\right)^{-1}\\boldsymbol u\\ ;\\ \\left(\\boldsymbol \\Omega^{(t)} + {\\boldsymbol K_{\\boldsymbol f, \\boldsymbol f}^{(t)}}^{-1}\\right)^{-1}\\right),\n",
    "$$\n",
    "$$\n",
    "\\boldsymbol K_{\\boldsymbol f, \\boldsymbol f}^{(t)}= \\left( k((X_i^{(t)}, Y_i^{(t)}), (X_j^{(t)}, Y_j^{(t)})^{(t)}) + U(X_i^{(t)}, Y_i^{(t)})^\\top\\ \\Sigma_\\epsilon\\ U(X_j^{(t)}, Y_j^{(t)}) \\right)_{\\substack{i=1:N_{tot}^{(t)}\\\\ j=1:N_{tot}^{(t)}}}\n",
    "$$\n",
    "$$\n",
    "\\boldsymbol X^{(t)}, \\boldsymbol Y^{(t)} = D \\cup \\Pi^{(t)}.\n",
    "$$\n",
    "\n",
    "Note that the prior mean has been set to zero, while a non-stationary term has been added to the GP covariance kernel. -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d3ab25-8fc6-4262-9c72-857f1cce63e7",
   "metadata": {},
   "source": [
    "### Updating the latent PP... and corresponding GP marginals!\n",
    "\n",
    "Here we introduce the following *auxiliary* latent variables:\n",
    "- $\\Pi^\\ast = ((X_i^\\ast, Y_i^\\ast))_{1\\leq i\\leq N^\\ast})$  the realization of a spatially and temporally homogeneous Poisson process over spatial domain $\\Omega$ and period of time $T$, with intensity $\\bar\\lambda.$ This means that:\n",
    "$$\n",
    "N^\\ast \\sim \\mathcal P(\\bar\\lambda T \\vert \\Omega\\vert)\n",
    "$$\n",
    "$$\n",
    "X_i^\\ast, Y_i^\\ast \\vert N^\\ast \\stackrel{iid}{\\sim} \\mathcal U(\\Omega),\\quad i=1,\\ldots, N^\\ast\n",
    "$$\n",
    "- $\\boldsymbol f^\\ast = (f(X_i^\\ast, Y_i^\\ast))_{1\\leq i\\leq N^\\ast}$ the GP evaluated at the auxiliary set of points. The full conditional density of this block (given all the other blocks) is multivariate normal, following the well-known \"sandwich\" formula:\n",
    "$$\n",
    "\\boldsymbol f^\\ast  \\vert \\boldsymbol f^{(t+1)}, \\Pi^{(t)}, \\Pi^\\ast, D \\sim \\mathcal N\\left(\n",
    "\\boldsymbol m^\\ast + \\boldsymbol K^{(t)}_{\\boldsymbol f^\\ast,\\boldsymbol f} {\\boldsymbol K_{\\boldsymbol f, \\boldsymbol f}^{(t)}}^{-1} (\\boldsymbol f^{(t+1)} - \\boldsymbol m{(t)})\\ ;\\ \\boldsymbol K_{\\boldsymbol f^\\ast,\\boldsymbol f^\\ast} - \\boldsymbol K_{\\boldsymbol f^\\ast,\\boldsymbol f}^{(t)} {\\boldsymbol K_{\\boldsymbol f, \\boldsymbol f}^{(t)}}^{-1}  \\boldsymbol K_{\\boldsymbol f,\\boldsymbol f^\\ast}^{(t)} \\right),\n",
    "$$\n",
    "with\n",
    "$$\n",
    "\\boldsymbol K_{\\boldsymbol f^\\ast, \\boldsymbol f}^{(t)} = \\left({\\boldsymbol K_{\\boldsymbol f, \\boldsymbol f^\\ast}^{(t)}}\\right)^\\top = \\left( k((X_i^\\ast, Y_i^\\ast), (X_j^{(t)}, Y_j^{(t)})^{(t)}) \\right)_{\\substack{i=1:N^\\ast\\\\ j=1:N_{tot}^{(t)}}}.\n",
    "$$\n",
    "\n",
    "$\\Pi^{(t+1)}$ is then recovered based on $\\Pi^\\ast$ and $\\boldsymbol f^\\ast$ through a *thinning* argument, meaning that each point $(X_i^\\ast, Y_i^\\ast)$ for $i=1,\\ldots, N^\\ast$ is allocated to $\\Pi^{(t+1)}$ with probability: $\\sigma (-f_i^\\ast),$ where $f_i^\\ast:=f(X_i^\\ast, Y_i^\\ast).$\n",
    "\n",
    "To summarize, the conditional density $\\Pi^{(t+1)} \\vert D, \\boldsymbol f^{(t+1)}$ is obtained as a marginal of the joint posterior conditional \n",
    "$$\n",
    "\\left[\\Pi^{(t+1)}, \\boldsymbol f^\\ast, \\Pi^\\ast \\vert D, \\boldsymbol f^{(t+1)}, \\boldsymbol \\Pi^{(t)} \\right] = \n",
    "\\left[\\Pi^{(t+1)} \\vert \\boldsymbol f^\\ast, \\Pi^\\ast  \\right] \\times\n",
    "\\left[\\boldsymbol f^\\ast  \\vert \\Pi^\\ast,  \\boldsymbol f^{(t+1)}, \\Pi^{(t)} \\right] \\times\n",
    "\\left[\\Pi^\\ast \\right]\n",
    "$$ \n",
    "\n",
    "which can be simulated by thinning, using the following steps:\n",
    "1. Sample the total number of points of the latent homogeneous Poisson process: $N^\\ast \\vert D \\sim \\mathcal P \\left(  \\bar\\lambda \\times T \\times \\vert \\Omega \\vert \\right).$\n",
    "2. For $i=1, \\ldots, N^\\ast$: simulate $(X_i^\\ast, Y_i^\\ast)$ uniformly over $\\Omega,$ yielding $\\Pi^\\ast$\n",
    "3. Simulate $\\boldsymbol f^\\ast$ using the above conditional density $\\boldsymbol f^\\ast  \\vert \\boldsymbol f^{(t+1)},  \\Pi^\\ast, \\Pi^{(t)}$\n",
    "4. For $i=1, \\ldots, N^\\ast$: allocate point $(X_i^\\ast, Y_i^\\ast)$ to $\\Pi^{(t+1)}$ with probability: $1-\\sigma (f_i^\\ast) = \\sigma (-f_i^\\ast)$;\n",
    "6. Modify accordingly: $N_{tot}^{(t+1)} := N + \\vert\\Pi^{(t+1)}\\vert$ and $\\boldsymbol f_\\Pi^{(t+1)}$ in $\\boldsymbol f^{(t+1)} = \\boldsymbol f_D^{(t+1)}\\cup\\boldsymbol f_\\Pi^{(t+1)},$ from its current value $\\boldsymbol f^{(t+1)}_{\\Pi^{(t)}}=(f^{(t+1)}(X_i^{(t)}, Y_i^{(t)}))_{i=N+1,\\ldots,N_{tot}^{(t)}}$ to its new value $\\boldsymbol f^{(t+1)}_{\\Pi^{(t+1)}} = (f_i^\\ast)_{i=N+1,\\ldots,N_{tot}^{(t+1)}}.$\n",
    "\n",
    "<!-- #### Incorporating prior information\n",
    "  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3d794b-ead6-4412-bd44-6c08b4bdc444",
   "metadata": {},
   "source": [
    "### Updating the PGP at all points (observed & latent)\n",
    "\n",
    "The posterior conditional density of $w_i^{(t+1)}$ for $i=1, \\ldots, N_{tot}$ is given by:\n",
    "$$\n",
    "\\omega_i^{(t+1)} \\vert f_i^{(t+1)} \\stackrel{ind}{\\sim} PG(1, \\vert f_i^{(t+1)}\\vert).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5543193b",
   "metadata": {},
   "source": [
    "### Updating the zone's effects'\n",
    "\n",
    "$\\boldsymbol \\epsilon$'s posterior conditional is Gaussian, with conditional mean and covariance:\n",
    "$$\n",
    "\\boldsymbol \\epsilon^{(t+1)} \\vert \\boldsymbol f^{(t+1)}, D, \\boldsymbol \\Pi^{(t+1)} \\sim \\mathcal N\\left( \n",
    "\\left( \\Sigma_\\epsilon^{-1} + {\\boldsymbol U^{(t+1)}}^\\top {K^{(t+1)}_{\\boldsymbol f, \\boldsymbol f}}^{-1} \\boldsymbol U^{(t+1)}\\right)^{-1} {\\boldsymbol U^{(t+1)}}^\\top {K^{(t+1)}_{\\boldsymbol f, \\boldsymbol f}}^{-1}\\boldsymbol f^{(t+1)}\\ ;\\\n",
    "\\left( \\Sigma_\\epsilon^{-1} + {\\boldsymbol U^{(t+1)}}^\\top {K^{(t+1)}_{\\boldsymbol f, \\boldsymbol f}}^{-1} \\boldsymbol U^{(t+1)}\\right)^{-1}\n",
    "\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e17b679-d4c7-44f8-9584-aa3def509e56",
   "metadata": {},
   "source": [
    "## Implementation details\n",
    "\n",
    "### Dimension of the MCMC chain\n",
    "\n",
    "We note $\\boldsymbol\\theta^{(t)}:=\\left(\\boldsymbol f^{(t)}, \\boldsymbol \\omega^{(t)}, \\Pi^{(t)}, N_{tot}^{(t)}, \\boldsymbol \\varepsilon^{(t)} \\right)_{1\\leq t\\leq T}$ the MCMC chain generated by the Gibbs sampler. Note that, at each step $t$, the chain has dimension: $d^{(t)} = 4 N_{tot}^{(t)} - 2N  + 1 + J.$\n",
    "\n",
    "However, using Open TURNS, the dimension of the MCMC chain must be fixed in advance. Therefore, we need to choose a reasonable upper bound for $N_{tot}^{(t)},$ let's call it $N_ {max}.$ Remember that in principle $N_{tot}$'s law is close to the Poisson $\\mathcal P\\left( \\bar\\lambda \\times T \\times \\vert \\Omega \\vert \\right),$ hence we suggest using the $99.9$-th percentile of this law.\n",
    "\n",
    "Open TURNS also requires the Gibbs blocks to be allocated once and for all, meaning that the blocks do not change from one iteration to the other. To do this, we must pre-allocate components of the parameter vector $\\boldsymbol\\theta$ to each block. We chose the following allocation policy:\n",
    "$$\n",
    "\\boldsymbol\\theta_{1:N_{ {max}}} := \\boldsymbol f \n",
    "$$\n",
    "$$\n",
    "\\boldsymbol\\theta_{N_{ {max}}+1:2N_{ {max}}} := \\boldsymbol \\omega \n",
    "$$\n",
    "$$\n",
    "\\boldsymbol\\theta_{2N_ {max}+1:4N_ {max}-2N} := \\Pi \n",
    "$$\n",
    "$$\n",
    "\\theta_{4N_ {max} - 2N+1} :=  N_{tot}\n",
    "$$\n",
    "$$\n",
    "\\boldsymbol\\theta_{4N_{max}-2N + 1 + 1: 4N_{max}-2N + 1 + J} := \\boldsymbol \\epsilon \n",
    "$$\n",
    "\n",
    "This relies on the assumption that, for all $t=1,\\ldots, T$ we have: $N_{tot}^{(t)} \\leq N_ {max},$ so that the last $N_ {max} - N_{tot}^{(t)}$ components of the latent variables $\\Pi$, $\\boldsymbol f_\\Pi$ and $\\boldsymbol \\omega_\\Pi$ may take arbitrary values. On the other hand, if per chance $N_{tot}^{(t)} > N_ {max},$ the latent processes are truncated, which may induce some residual bias on the final estimation, showing that some care must be taken when calibrating $N_ {max}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1891eaf5-c2fb-4222-b81b-06b90adba7e9",
   "metadata": {},
   "source": [
    "\n",
    "### Efficient conditional Normal simulation\n",
    "\n",
    "$$\n",
    "\\boldsymbol f \\vert D, \\Pi, \\boldsymbol \\omega \\sim \\mathcal N\\left(\\left(\\boldsymbol \\Omega + \\boldsymbol K_{\\boldsymbol f, \\boldsymbol f}^{-1}\\right)^{-1}\\left( \\boldsymbol K_{\\boldsymbol f, \\boldsymbol f}^{-1} \\boldsymbol m + \\boldsymbol u\\ \\right)\\ ;\\ \\left(\\boldsymbol \\Omega + \\boldsymbol K_{\\boldsymbol f, \\boldsymbol f}^{-1}\\right)^{-1}\\right),\n",
    "$$\n",
    "with $\\boldsymbol \\Omega = diag\\left(\\boldsymbol\\omega\\right),$ and $\\boldsymbol u := (u_1, \\ldots, u_{N_{tot}}),$ $u_i = \\frac{1}{2}$ if $i \\leq N$ and $u_i = -\\frac{1}{2}$ if $i > N.$\n",
    "\n",
    "Let $LL^\\top = \\boldsymbol K_{\\boldsymbol f, \\boldsymbol f}$ be $\\boldsymbol K_{\\boldsymbol f, \\boldsymbol f}$'s Cholesky decomposition, so that: ${L^{-1}}^\\top L^{-1} = \\boldsymbol K_{\\boldsymbol f, \\boldsymbol f}^{-1}$\n",
    "\n",
    "Then, owing to Woodbury's formula:\n",
    "$$\n",
    "\\left(\\boldsymbol \\Omega + \\boldsymbol K_{\\boldsymbol f, \\boldsymbol f}^{-1}\\right)^{-1} = \n",
    "\\left(\\boldsymbol \\Omega + {L^{-1}}^\\top L^{-1}\\right)^{-1} = \n",
    "\\boldsymbol\\Omega^{-1} - \\boldsymbol\\Omega^{-1} \n",
    "\\left( \n",
    "\\boldsymbol I_{N_{tot}} + L^{-1}\\boldsymbol\\Omega^{-1}{L^{-1}}^\\top\n",
    "\\right)^{-1}\n",
    "\\boldsymbol\\Omega^{-1}\n",
    "$$\n",
    "\n",
    "Remember that, to simulate $X \\sim \\mathcal N(\\mu, \\Sigma),$ with $\\mu \\in \\mathbb R^d,$ $\\Sigma \\in \\mathbb R^{d}\\times\\mathbb R^{d}$ def. pos., a numerically sensible strategy is to notice that:\n",
    "$$\n",
    "Z := \\Sigma^{-1/2}(X - \\mu) \\sim \\mathcal N(0, I_d),\n",
    "$$\n",
    "and that $\\Sigma^{-1/2} = L,$ with $L$ the Cholesky decomposition of $\\Sigma^{-1}$ such that: $\\Sigma^{-1}=LL^\\top.$ Hence $X$ can be simulated as:\n",
    "$$\n",
    "X := L^{-1}Z + \\mu\n",
    "$$\n",
    "In the current case this implies $LL^\\top = \\Sigma^{-1} = \\boldsymbol \\Omega + \\boldsymbol K_{\\boldsymbol f, \\boldsymbol f}^{-1}.$ Besides we also have:\n",
    "$$\n",
    "\\mu = \\Sigma\\left(\\boldsymbol K_{\\boldsymbol f, \\boldsymbol f}^{-1} \\boldsymbol m + \\boldsymbol u\\right)\n",
    "= (L^{-1})^\\top L^{-1}\\left(\\boldsymbol K_{\\boldsymbol f, \\boldsymbol f}^{-1} \\boldsymbol m + \\boldsymbol u\\right)\n",
    "$$\n",
    "So that finally:\n",
    "$$\n",
    "X := L^{-1}Z + (L^{-1})^\\top L^{-1}\\left(\\boldsymbol K_{\\boldsymbol f, \\boldsymbol f}^{-1} \\boldsymbol m + \\boldsymbol u\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f79fea7",
   "metadata": {},
   "source": [
    "Likewise, to simulate from the posterior conditional density of $\\boldsymbol \\epsilon,$ given by:\n",
    "$$\n",
    "\\boldsymbol \\epsilon^{(t+1)} \\vert \\boldsymbol f^{(t+1)}, D, \\boldsymbol \\Pi^{(t+1)} \\sim \\mathcal N\\left( \n",
    "\\left( \\Sigma_\\epsilon^{-1} + {\\boldsymbol U^{(t+1)}}^\\top {K^{(t+1)}_{\\boldsymbol f, \\boldsymbol f}}^{-1} \\boldsymbol U^{(t+1)}\\right)^{-1} {\\boldsymbol U^{(t+1)}}^\\top {K^{(t+1)}_{\\boldsymbol f, \\boldsymbol f}}^{-1} \\boldsymbol f^{(t+1)}\\ ;\\\n",
    "\\left( \\Sigma_\\epsilon^{-1} + {\\boldsymbol U^{(t+1)}}^\\top {K^{(t+1)}_{\\boldsymbol f, \\boldsymbol f}}^{-1} \\boldsymbol U^{(t+1)}\\right)^{-1}\n",
    "\\right),\n",
    "$$\n",
    "one needs to compute the Cholesky decompositions $J J^\\top = K^{(t+1)}_{\\boldsymbol f, \\boldsymbol f}$ and $L L^\\top = \\left( \\Sigma_\\epsilon^{-1} + {\\boldsymbol U^{(t+1)}}^\\top {K^{(t+1)}_{\\boldsymbol f, \\boldsymbol f}}^{-1} \\boldsymbol U^{(t+1)}\\right)$ of $\\boldsymbol \\epsilon$'s precision matrix, then simulate Gaussian white noise $Z \\sim \\mathcal N\\left( \\boldsymbol 0_J\\ ;\\ \\boldsymbol I_J\\right),$ and then compute\n",
    "$$\n",
    "\\boldsymbol \\epsilon^{(t+1)} \\vert \\boldsymbol f^{(t+1)}, D, \\boldsymbol \\Pi^{(t+1)} \\stackrel{\\mathcal L}{=} L^{-1}Z + (L^{-1})^\\top L^{-1} {\\boldsymbol U^{(t+1)}}^\\top {J^{-1}}^\\top J^{-1} \\boldsymbol f^{(t+1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48fae14a-fbaf-4bba-8c79-bd66c465e8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/H01971/miniforge3/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "#    Necessary imports   #\n",
    "##########################\n",
    "\n",
    "import os\n",
    "import openturns as ot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import time\n",
    "import scipy.optimize as so\n",
    "import scipy.stats as st\n",
    "import statsmodels.tsa.stattools as stattools\n",
    "from polyagamma import random_polyagamma\n",
    "\n",
    "ot.RandomGenerator.SetSeed(0) # Make results reproducible by freezing Open TURNS's random generator's seed\n",
    "np.random.seed(0) # Make results reproducible by freezing Numpy's random generator's seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9f48ae5-058e-4e20-8e78-79bbf9ca1c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Simulate data #\n",
    "#################\n",
    "\n",
    "# Assuming square domain [0,1]*[0,1] (surface 1)\n",
    "# and null trend\n",
    "\n",
    "lambdaBar = 100\n",
    "T = 10\n",
    "\n",
    "sigmoid = ot.SymbolicFunction(['z'], ['1/(1+exp(-z))'])\n",
    "def trend(X):\n",
    "    return [0]\n",
    "\n",
    "cov = ot.SquaredExponential([0.5, 0.5], [1.0])\n",
    "m = ot.PythonFunction(2, 1, trend)\n",
    "\n",
    "# Use thinning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "798e2600-4cd0-4071-a0c6-5add7467ff3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1034\n"
     ]
    }
   ],
   "source": [
    "N_star = int(ot.Poisson(lambdaBar * T).getRealization()[0])\n",
    "print(N_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d630faf9-9d24-43cd-b95f-24163ea2f0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "  <tr><td></td><th>v0</th><th>v1</th><th>y0</th></tr>\n",
       "  <tr><th>0</th><td>-0.7294473</td><td>0.6944882</td><td>0.1547132</td></tr>\n",
       "  <tr><th>1</th><td>-0.9349945</td><td>0.7525542</td><td>0.2271906</td></tr>\n",
       "  <tr><th>2</th><td>-0.3058859</td><td>-0.1990934</td><td>0.4166959</td></tr>\n",
       "<tr><td colspan=\"4\">...</td></tr>\n",
       "  <tr><th>1031</th><td>-0.3194695</td><td>-0.8331369</td><td>0.2072398</td></tr>\n",
       "  <tr><th>1032</th><td>-0.3335571</td><td>-0.3950307</td><td>0.4400152</td></tr>\n",
       "  <tr><th>1033</th><td>-0.4738648</td><td>0.403297</td><td>0.1448101</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "class=Field name=Unnamed description=[v0,v1,y0] implementation=class=FieldImplementation name=Unnamed mesh=class=Mesh name=Unnamed dimension=2 vertices=class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=1034 dimension=2 data=[[-0.729447,0.694488],[-0.934994,0.752554],[-0.305886,-0.199093],...,[-0.319469,-0.833137],[-0.333557,-0.395031],[-0.473865,0.403297]] simplices=[] values=class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=1034 dimension=1 description=[y0] data=[[0.154713],[0.227191],[0.416696],...,[0.20724],[0.440015],[0.14481]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XY_star = np.array(ot.ComposedDistribution([ot.Uniform()]*2).getSample(N_star))\n",
    "mesh = ot.Mesh(XY_star)\n",
    "mTrend = ot.TrendTransform(m, mesh)\n",
    "F = ot.GaussianProcess(mTrend, cov, mesh)\n",
    "field_function = ot.PythonFieldFunction(mesh, 1, mesh, 1, sigmoid)\n",
    "process = ot.CompositeProcess(field_function, F)     \n",
    "field_f = process.getRealization()\n",
    "field_f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
